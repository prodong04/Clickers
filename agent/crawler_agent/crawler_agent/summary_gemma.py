import re
import torch
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_core.prompts import PromptTemplate


def is_numeric_line(line):
    """
    ì¤„ ì „ì²´ê°€ ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ íŒë³„í•œë‹¤.
    í—ˆìš©í•˜ëŠ” ë¬¸ìëŠ” ìˆ«ì, ì†Œìˆ˜ì (.), ì½¤ë§ˆ(,), ìŒìˆ˜ ê¸°í˜¸(-), í¼ì„¼íŠ¸(%)
    ê·¸ë¦¬ê³  ê²½ìš°ì— ë”°ë¼ ì•ì— 'n' ë˜ëŠ” 'N'ì´ ì˜¬ ìˆ˜ ìˆë‹¤.
    """
    return re.fullmatch(r'[nN]?[ \t]*[-\d\.,%]+', line.strip()) is not None


def remove_tables_and_numbers(text):
    """ í…Œì´ë¸” ë° ë¶ˆí•„ìš”í•œ ìˆ«ì ë°ì´í„°ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜ """
    # í…Œì´ë¸” ê°ì§€ ë° ì‚­ì œ (ìˆ«ìê°€ ì—°ì†ë˜ëŠ” í–‰ íŒ¨í„´)
    text = re.sub(r"(\d+[\t ]+\d+[\t ]+\d+[\t ]+\d+[\t ]*\d*)", "", text)  # ìˆ«ìê°€ ë§ì€ í–‰ ì‚­ì œ
    text = re.sub(r"(\d{2,}[-/.]\d{2,}[-/.]\d{2,})", "", text)  # ë‚ ì§œ í˜•ì‹ ì œê±°
    text = re.sub(r"\d{2,}%", "", text)  # í¼ì„¼íŠ¸ ê°’ ì‚­ì œ
    text = re.sub(r"\d{5,}", "", text)  # ë„ˆë¬´ ê¸´ ìˆ«ì(ìš°í¸ë²ˆí˜¸ ë“±) ì‚­ì œ
    return text.strip()


def clean_response(response,prompt_text):
    """ ëª¨ë¸ ì¶œë ¥ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±° """
    response = response.strip()
    response = re.sub(r"\[INST\].*?\[/INST\]", "", response, flags=re.DOTALL)
    response = re.sub(r"<\|assistant\|>", "", response).strip()
    response = response.replace(prompt_text, "").strip()
    response = response.replace("*", "").strip()
    return response


def summarization(fname, category="stock", model_name="rtzr/ko-gemma-2-9b-it", temperature=0.7):
    # PDF ë¡œë”©
    loader = PyMuPDFLoader(fname)
    docs = loader.load()

    # ë¬¸ì„œ ë¶„í•  (ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=500)
    split_documents = text_splitter.split_documents(docs)

    content = []

    # ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ì¤„ ì œê±° + í…Œì´ë¸” ì œê±°
    for doc in split_documents:
        lines = doc.page_content.splitlines()
        filtered_lines = [line for line in lines if not is_numeric_line(line)]
        doc.page_content = remove_tables_and_numbers("\n".join(filtered_lines))

    for s in split_documents:
        content.append(s.page_content)

    # **í•œ ë²ˆì— ëª¨ë“  contentë¥¼ ì…ë ¥**
    full_text = "\n\n".join(content)

    # **Qwen ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ ì„¤ì • (í…Œì´ë¸” ì‚­ì œ ì§€ì‹œ ì¶”ê°€)**

    stock_prompt = PromptTemplate.from_template(
    """Based on the provided document, reformat the text according to the following guidelines:

    - Do not omit any content from the main body, including all numerical details, percentages, and units.
    - If sections such as "[ Compliance Notice ]", "[ ì¢…ëª© íˆ¬ìë“±ê¸‰ ]", "[ ì‚°ì—… íˆ¬ìì˜ê²¬ ]" are present, they may be omitted.
    - **Remove tables and meaningless sequences of numbers before processing.**
    
    Output the result in the following format:
    0. ì¢…ëª© í‚¤ì›Œë“œ: [extracted keywords]
    1. ì¢…ëª© ì„¤ëª…: [detailed description of the stock]
    2. ë‚´ìš©: [reformatted document content]
    3. ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‚¬ì : [analyst's insights]
    4. ì‹œì‚¬ì ì— ëŒ€í•œ ê·¼ê±°: [supporting evidence]

    Document:
    {split_documents}"""
)

    sector_prompt = PromptTemplate.from_template(
        """Based on the provided document, reformat the text according to the following guidelines:

    - Do not omit any content from the main body, including all numerical details, percentages, and units.
    - If sections such as "[ Compliance Notice ]", "[ ì¢…ëª© íˆ¬ìë“±ê¸‰ ]", "[ ì‚°ì—… íˆ¬ìì˜ê²¬ ]" are present, they may be omitted.
    - **Remove tables and meaningless sequences of numbers before processing.**
    
    Output the result in the following format:
    0. ì¢…ëª© í‚¤ì›Œë“œ: [extracted keywords]
    1. ë‚´ìš©: [reformatted document content]
    2. ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‚¬ì : [analyst's insights]
    3. ì‹œì‚¬ì ì— ëŒ€í•œ ê·¼ê±°: [supporting evidence]

    Document:
    {split_documents}"""
    )

    # Gemma ëª¨ë¸ ë¡œë“œ
    prompt = stock_prompt if category == "stock" else sector_prompt

    # **ğŸ’¡ ëª¨ë¸ ë¡œë“œ ìµœì í™” (FP16 ê°•ì œ + ë‹¤ì¤‘ GPU)**
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",  # ìë™ìœ¼ë¡œ ì—¬ëŸ¬ GPUì— ë°°ì¹˜
        torch_dtype=torch.float16,  # FP16 ê°•ì œ ì ìš©
        offload_folder="offload",  # ë¶€ì¡±í•œ ê²½ìš° CPU ì˜¤í”„ë¡œë”©
        use_cache=True  # ìºì‹± í™œì„±í™”
    )

    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        batch_size=32,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (ì†ë„ í–¥ìƒ)
    )

    # **ğŸ’¡ ì†ë„ ë° ì¶œë ¥ ìµœì í™”**
    response_list = []
    # for content_chunk in content:
    prompt_text = prompt.format(split_documents=content)
    # print(content)

    result = generator(
        prompt_text,
        max_new_tokens=2048,  # ğŸ”¥ (2048 â†’ 768)ë¡œ ìµœì í™” (ì†ë„ ì¦ê°€)
        temperature=temperature,
        do_sample=True,  # ğŸ”¥ ìƒ˜í”Œë§ í™œì„±í™” (ì°½ì˜ì ì¸ ìš”ì•½)
        top_p=0.9,  # ğŸ”¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë§Œ ì„ íƒ
        use_cache=True,
        num_return_sequences=1
    )

    response_text = result[0]["generated_text"]
    cleaned_text = clean_response(response_text, prompt_text)
    response_list.append(cleaned_text)
    result = "\n\n".join(response_list)

    return result
