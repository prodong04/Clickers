import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_core.prompts import PromptTemplate


def is_numeric_line(line):
    """
    ì¤„ ì „ì²´ê°€ ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ì§€ íŒë³„í•œë‹¤.
    í—ˆìš©í•˜ëŠ” ë¬¸ìëŠ” ìˆ«ì, ì†Œìˆ˜ì (.), ì½¤ë§ˆ(,), ìŒìˆ˜ ê¸°í˜¸(-), í¼ì„¼íŠ¸(%)
    ê·¸ë¦¬ê³  ê²½ìš°ì— ë”°ë¼ ì•ì— 'n' ë˜ëŠ” 'N'ì´ ì˜¬ ìˆ˜ ìˆë‹¤.
    """
    return re.fullmatch(r'[nN]?[ \t]*[-\d\.,%]+', line.strip()) is not None


def remove_tables_and_numbers(text):
    """ í…Œì´ë¸” ë° ë¶ˆí•„ìš”í•œ ìˆ«ì ë°ì´í„°ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜ """
    # í…Œì´ë¸” ê°ì§€ ë° ì‚­ì œ (ìˆ«ìê°€ ì—°ì†ë˜ëŠ” í–‰ íŒ¨í„´)
    text = re.sub(r"(\d+[\t ]+\d+[\t ]+\d+[\t ]+\d+[\t ]*\d*)", "", text)  # ìˆ«ìê°€ ë§ì€ í–‰ ì‚­ì œ
    text = re.sub(r"(\d{2,}[-/.]\d{2,}[-/.]\d{2,})", "", text)  # ë‚ ì§œ í˜•ì‹ ì œê±°
    text = re.sub(r"\d{2,}%", "", text)  # í¼ì„¼íŠ¸ ê°’ ì‚­ì œ
    text = re.sub(r"\d{5,}", "", text)  # ë„ˆë¬´ ê¸´ ìˆ«ì(ìš°í¸ë²ˆí˜¸ ë“±) ì‚­ì œ
    return text.strip()


def clean_response(response):
    """ ëª¨ë¸ ì¶œë ¥ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±° """
    response = response.strip()
    response = re.sub(r"\[INST\].*?\[/INST\]", "", response, flags=re.DOTALL)
    response = re.sub(r"<\|assistant\|>", "", response).strip()
    return response


def summarization(fname, category="stock", model_name="Qwen/Qwen2.5-7B-Instruct-1M", temperature=0.7):
    # PDF ë¡œë”©
    loader = PyMuPDFLoader(fname)
    docs = loader.load()

    # ë¬¸ì„œ ë¶„í•  (ë„ˆë¬´ ê¸´ ì…ë ¥ ë°©ì§€)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=500)
    split_documents = text_splitter.split_documents(docs)

    content = []

    # ìˆ«ìë¡œë§Œ ì´ë£¨ì–´ì§„ ì¤„ ì œê±° + í…Œì´ë¸” ì œê±°
    for doc in split_documents:
        lines = doc.page_content.splitlines()
        filtered_lines = [line for line in lines if not is_numeric_line(line)]
        doc.page_content = remove_tables_and_numbers("\n".join(filtered_lines))

    for s in split_documents:
        content.append(s.page_content)

    # **í•œ ë²ˆì— ëª¨ë“  contentë¥¼ ì…ë ¥**
    full_text = "\n\n".join(content)

    # **Qwen ëª¨ë¸ìš© í”„ë¡¬í”„íŠ¸ ì„¤ì • (í…Œì´ë¸” ì‚­ì œ ì§€ì‹œ ì¶”ê°€)**

    stock_prompt = PromptTemplate.from_template(
    """Based on the provided document, reformat the text according to the following guidelines:

    - Do not omit any content from the main body, including all numerical details, percentages, and units.
    - If sections such as "[ Compliance Notice ]", "[ ì¢…ëª© íˆ¬ìë“±ê¸‰ ]", "[ ì‚°ì—… íˆ¬ìì˜ê²¬ ]" are present, they may be omitted.
    - **Remove tables and meaningless sequences of numbers before processing.**
    
    Output the result in the following format:
    0. ì¢…ëª© í‚¤ì›Œë“œ: [extracted keywords]
    1. ì¢…ëª© ì„¤ëª…: [detailed description of the stock]
    2. ë‚´ìš©: [reformatted document content]
    3. ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‚¬ì : [analyst's insights]
    4. ì‹œì‚¬ì ì— ëŒ€í•œ ê·¼ê±°: [supporting evidence]

    Document:
    {split_documents}"""
)

    sector_prompt = PromptTemplate.from_template(
        """Based on the provided document, reformat the text according to the following guidelines:

    - Do not omit any content from the main body, including all numerical details, percentages, and units.
    - If sections such as "[ Compliance Notice ]", "[ ì¢…ëª© íˆ¬ìë“±ê¸‰ ]", "[ ì‚°ì—… íˆ¬ìì˜ê²¬ ]" are present, they may be omitted.
    - **Remove tables and meaningless sequences of numbers before processing.**
    
    Output the result in the following format:
    0. ì¢…ëª© í‚¤ì›Œë“œ: [extracted keywords]
    1. ë‚´ìš©: [reformatted document content]
    2. ì• ë„ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‚¬ì : [analyst's insights]
    3. ì‹œì‚¬ì ì— ëŒ€í•œ ê·¼ê±°: [supporting evidence]

    Document:
    {split_documents}"""
    )

    prompt_template = stock_prompt if category == "stock" else sector_prompt

    prompt_text = prompt_template.format(split_documents=full_text)

    # Qwen ëª¨ë¸ ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,  # FP16 ì ìš© (ì†ë„ & ë©”ëª¨ë¦¬ ìµœì í™”)
        device_map="auto",  # ìë™ GPU ë°°ì¹˜
    )

    # Qwen ëª¨ë¸ì€ `messages` í˜•ì‹ í•„ìš”
    messages = [
        {"role": "system", "content": "You are Qwen, an AI assistant specializing in document summarization. **Do not include tables or meaningless sequences of numbers in your output.**"},
        {"role": "user", "content": prompt_text},
    ]

    # Qwen ëª¨ë¸ì˜ ì±„íŒ… í…œí”Œë¦¿ ì ìš©
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

    # **ìµœëŒ€ í† í° ì„¤ì • (8192ê¹Œì§€ í—ˆìš©)**
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=4096,  # ğŸ”¥ ìµœëŒ€ 4096 í† í°ê¹Œì§€ ì¶œë ¥ ê°€ëŠ¥
        temperature=temperature,
        do_sample=True,  # ìƒ˜í”Œë§ í™œì„±í™”
        top_p=0.9,  # ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë§Œ ì„ íƒ
    )

    # ì¶œë ¥ ì •ì œ
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    cleaned_text = clean_response(response_text)

    return cleaned_text